clear;warning('off', 'Octave:possible-matlab-short-circuit-operator');load('ex4data1.mat');load('ex4weights.mat');%Theta 1 es de dimensión 25 x 401%Theta 2 es de dimensión 10 x 26% La visualización de los datos ya se ha realizado en la práctica anterior, por % por lo que se obviará en esta práctica %Se inicializan los parámetros:params_rn = [Theta1(:) ; Theta2(:)];lambda = 1;num_entradas  = 400;num_ocultas = 25;num_etiquetas = 10;  %========== PARTE 1, 2 ================[J, grad] = costeRN(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, lambda);checkNNGradients(lambda);% Tras completar el 2.3% >>script; %The above two columns you get should be very similar.%(Left-Your Numerical Gradient, Right-Analytical Gradient)%If your backpropagation implementation is correct, then%the relative difference will be small (less than 1e-9).%Relative Difference: 2.29153e-11%========== PARTE 3 ===================options = optimset('MaxIter', 50);lambda = 1;r_theta1 = pesosAleatorios(num_entradas, num_ocultas);r_theta2 = pesosAleatorios(num_ocultas, num_etiquetas);param = [r_theta1(:) ; r_theta2(:)];theta = fmincg(@(t)(costeRN(param, num_entradas, num_ocultas,   num_etiquetas, X, y, lambda)), param, options);t1 = reshape(theta(1:num_ocultas * (num_entradas + 1)),                  num_ocultas, (num_entradas + 1));t2 = reshape(theta((1 + (num_ocultas * (num_entradas + 1))):end),                  num_etiquetas, (num_ocultas + 1));                 acc = acierto(t1, t2, X, y);fprintf('\nTraining Set Accuracy: %f%%\n', acc * 100);                 % /PROBAR DISTINTOS LAMBDAS/